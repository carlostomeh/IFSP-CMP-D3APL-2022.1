{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D3APL: Aplicações em Ciência de Dados** <br/>\n",
    "IFSP Campinas\n",
    "\n",
    "Prof. Dr. Samuel Martins (Samuka) <br/><br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Binary Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with GD and L2 Regularization (Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: use the lecture slides to support your development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "$$\n",
    "R(\\mathbf{w}) = \\alpha\\frac{1}{2}\\sum_{j=1}^{n}w_j^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing changes for the bias.\n",
    "$$\n",
    "b = b - \\eta \\sum_{i=1}^{m}\\left[(\\hat{p}^{(i)} - y^{(i)})\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\eta \\left( \\sum_{i=1}^{m}\\left[(\\hat{p}^{(i)} - y^{(i)}) x^{(i)}_j \\right] + \\alpha w_j \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./datasets/fake_train.npy')\n",
    "y_train = np.load('./datasets/fake_train_labels.npy')\n",
    "\n",
    "X_test = np.load('./datasets/fake_test.npy')\n",
    "y_test = np.load('./datasets/fake_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'y_train.shape = {y_train.shape}')\n",
    "\n",
    "print(f'X_test.shape = {X_test.shape}')\n",
    "print(f'y_test.shape = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=y_train)\n",
    "plt.xlabel('x1')\n",
    "plt.xlabel('x2')\n",
    "plt.title('Scatter plot: Training Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1], hue=y_test, marker='s')\n",
    "plt.xlabel('x1')\n",
    "plt.xlabel('x2')\n",
    "plt.title('Scatter plot: Testing Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "### Plan of Attack\n",
    "- Add the regularization parameter $\\alpha$ to the constructor\n",
    "- Change \\_\\_str\\_\\_\n",
    "- Change the gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y\n",
    "\n",
    "\n",
    "class LogisticRegression(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"Our Logistic Regression implemented from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate : float = 0.001,\n",
    "                 n_epochs : int = 1000, random_state : int = 42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        learning_rate : float, default=0.001\n",
    "            Learning rate.\n",
    "        n_epochs : int, default=1000\n",
    "            Number of epochs for training (convergence stop).\n",
    "        random_state : int, default=42\n",
    "            Seed used for generating random numbers.\n",
    "        \"\"\"\n",
    "        assert (learning_rate is not None) and (learning_rate > 0.0), \\\n",
    "        f'Learning rate must be > 0. Passed: {learning_rate}'\n",
    "\n",
    "        assert (n_epochs is not None) and n_epochs > 0, \\\n",
    "        f'Number of epochs must be > 0. Passed: {n_epochs}'\n",
    "        \n",
    "        assert (random_state is not None) and random_state >= 0, \\\n",
    "        f'Random state should be >= 0. Passed: {random_state}'\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # model / parameters - private\n",
    "        self.__w = None\n",
    "        # bias\n",
    "        self.__b = None\n",
    "    \n",
    "    \n",
    "    # a special method used to represent a class object as a string, called with print() or str()\n",
    "    def __str__(self):\n",
    "        msg = f'Learning Rate: {self.learning_rate}\\n' \\\n",
    "              f'Number of Epochs: {self.n_epochs}\\n' \\\n",
    "              f'Random state (seed): {self.random_state}\\n\\n' \\\n",
    "              f'Trained?: {self.is_fitted()}\\n'\n",
    "        return msg\n",
    "    \n",
    "    \n",
    "    def is_fitted(self) -> bool:\n",
    "        \"\"\"Check if the estimator is fitted by verifying the presence of a (learned) weight matrix\"\"\"\n",
    "        return self.__w is not None\n",
    "\n",
    "    \n",
    "    # getter: access the function as an attribute\n",
    "    @property\n",
    "    def coef_(self) -> ndarray:\n",
    "        \"\"\"Return the weight matrix (learned parameters) if the estimator was fitted/trained.\n",
    "           Otherwise, raise an exception.\n",
    "        \"\"\"\n",
    "        assert self.is_fitted(), 'The instance is not fitted yet.'\n",
    "        return self.__w\n",
    "    \n",
    "    # getter: access the function as an attribute\n",
    "    @property\n",
    "    def intercept_(self) -> float:\n",
    "        \"\"\"Return the bias (learned intercepet) if the estimator was fitted/trained.\n",
    "           Otherwise, raise an exception.\n",
    "        \"\"\"\n",
    "        assert self.is_fitted(), 'The instance is not fitted yet.'\n",
    "        return self.__b\n",
    "\n",
    "    \n",
    "    def __sigmoid(self, z: float) -> float:\n",
    "        return 1 / (1 + np.e ** (-z))\n",
    "\n",
    "\n",
    "    def __log_loss(self, y: ndarray, p_hat: ndarray, eps: float = 1e-15):\n",
    "        \"\"\"Return the log loss for a given estimation and ground-truth (true labels).\n",
    "        \n",
    "        Since log loss is undefined for `p=0` and `p=1`, we clipped the probabilities to max(eps, min(1 - eps, p)).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            True labels of input samples.\n",
    "        p_hat : ndarray\n",
    "            Estimated probabilities of input samples.\n",
    "        eps : float, default=1e-15\n",
    "            Epsilon term used to avoid undefined log loss at 0 and 1.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        log_loss : float\n",
    "            Computed log loss.\n",
    "        \"\"\"\n",
    "        p_hat_eps = np.maximum(eps, np.minimum(1 - eps, p_hat))\n",
    "        \n",
    "        losses = -(y * np.log(p_hat_eps) + (1 - y) * np.log(1 - p_hat_eps))\n",
    "        log_loss = losses.mean()\n",
    "\n",
    "        return log_loss\n",
    "    \n",
    "    \n",
    "    def __gradient(self, X: ndarray, y: ndarray, p_hat: ndarray) -> Tuple[ndarray, float]:\n",
    "        '''Compute the gradient vector for the log loss with regards to the weights and bias.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y: ndarray of shape (n_samples,).\n",
    "            Target (true) labels.\n",
    "        p_hat : ndarray, shape (n_samples,)\n",
    "            Estimated probabilities.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[ndarray, float]: \n",
    "            Tuple with:\n",
    "            - a numpy array of shape (n_features,) containing the partial derivatives w.r.t. the weights; and\n",
    "            - a float representing the partial derivative w.r.t. the bias.\n",
    "        '''\n",
    "        # X.shape ==> (n_samples, n_features)\n",
    "        # y.shape ==> (n_samples,)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        error = p_hat - y  # shape (n_samples,)\n",
    "        \n",
    "        grad_w = np.dot(error, X) / n_samples  # shape (n_features,)\n",
    "        \n",
    "        # as x_0 will be array 1.0 (bias trick), the gradient for the bias is simplified\n",
    "        grad_b = np.sum(error) / n_samples  # scalar\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X: ndarray, y: ndarray, verbose: int = 0):\n",
    "        '''Train a Logistic Regression classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y: ndarray of shape (n_samples,).\n",
    "            Target (true) labels.\n",
    "        verbose: int, default=0\n",
    "            Verbose flag. Print training information every `verbose` iterations.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        '''\n",
    "        ### CHECK INPUT ARRAY DIMENSIONS\n",
    "        assert X.ndim == 2, f'X must be 2D. Passed: {X.ndim}'\n",
    "        assert y.ndim == 1, f'y must be 1D. Passed: {y.ndim}'\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "            f'X.shape[0] should be equal to y.shape[0], instead: {X.shape[0]} != {y.shape[0]}'\n",
    "        # X, y = check_X_y(X, y)\n",
    "\n",
    "        ### SETTING SEED\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Altough the bias trick is an elegant solution to merge all\n",
    "        # parameters, it requires to extend the feature matrix X\n",
    "        # by adding one column of 1.0. This demands time, so we will\n",
    "        # deal with the weights and bias separately.\n",
    "\n",
    "        ### PARAMETER INITIALIZATION\n",
    "        \n",
    "        # return values from the “standard normal” distribution.\n",
    "        w = np.random.randn(n_features)\n",
    "        b = 0.0\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        # learning iterations\n",
    "        for epoch in np.arange(self.n_epochs):\n",
    "            ### ESTIMATION (FORWARD PASS)\n",
    "            # X.shape == (n_samples, n_features)\n",
    "            # w.shape = (n_features,)\n",
    "            # p_hat = (n_samples,)\n",
    "            z = np.dot(X, w) + b\n",
    "            p_hat = self.__sigmoid(z)\n",
    "            \n",
    "            loss = self.__log_loss(y, p_hat)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            ### GRADIENT DESCENT (BACKWARD PASS)\n",
    "            # grad_w.shape ==> (n_samples, 1)\n",
    "            # grad_b: scalar\n",
    "            grad_w, grad_b = self.__gradient(X, y, p_hat)\n",
    "            w = w - self.learning_rate * grad_w\n",
    "            b = b - self.learning_rate * grad_b\n",
    "            \n",
    "            # check to see if an update should be displayed\n",
    "            if verbose and (epoch == 0 or (epoch + 1) % verbose == 0):\n",
    "                print(f'[INFO] epoch={epoch + 1}/{self.n_epochs}, loss={loss:.7f}')\n",
    "        \n",
    "        if verbose > 0:\n",
    "            losses = np.array(losses)\n",
    "            print(f'\\nFinal loss: {loss}')\n",
    "            print(f'\\nMean loss: {losses.mean()} +- {losses.std()}')\n",
    "            \n",
    "        ### ASSIGN THE TRAINED PARAMETERS TO THE PRIVATE ATTRIBUTES\n",
    "        self.__w = w\n",
    "        self.__b = b\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X: ndarray) -> ndarray:\n",
    "        '''Estimate the probability for the positive class of input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            The estimated probabilities for the positive class of input samples.\n",
    "        '''\n",
    "        assert self.is_fitted(), 'The instance is not fitted yet.'\n",
    "        assert X.ndim == 2, f'X must be 2D. Passed: {X.ndim}'\n",
    "        \n",
    "        z = np.dot(X, self.__w) + self.__b\n",
    "\n",
    "        return self.__sigmoid(z)\n",
    "\n",
    "    \n",
    "    def predict(self, X: ndarray) -> ndarray:\n",
    "        '''Predict the labels for input samples.\n",
    "        \n",
    "        Thresholding at probability >= 0.5.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            Predicted labels of input samples.\n",
    "        '''\n",
    "        assert self.is_fitted(), 'The instance is not fitted yet.'\n",
    "        assert X.ndim == 2, f'X must be 2D. Passed: {X.ndim}'\n",
    "\n",
    "        p_hat = self.predict_proba(X)\n",
    "        y_hat = p_hat >= 0.5  # ndarray of bools\n",
    "        y_hat = y_hat.astype(np.int)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing constructor and `__str__`**\n",
    "- evaluate the default hyperparameters\n",
    "- try different valid values for them\n",
    "- try invalid values for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "print('Printing object by print()')\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Displaying object')\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing `fit()`**\n",
    "PS: use `pdb.set_trace()` inside the main loop of `fit()` for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization - alpha=0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization - alpha=0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization - alpha=100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualizing the Decision Boundary**\n",
    "L2 regularization with alpha=100, n_epochs=10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_1x_1 + w_2x_2 + b = 0$\n",
    "\n",
    "$x_2 = -(b + w_1x_1)/w_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = clf.intercept_\n",
    "w1, w2 = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_decision_line = np.array([X_train[:,0].min(), X_train[:,0].max()])\n",
    "x2_decision_line = -(b + (w1 * x1_decision_line)) / w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=y_train)\n",
    "sns.lineplot(x=x1_decision_line, y=x2_decision_line, color='lightseagreen')\n",
    "plt.xlabel('x1')\n",
    "plt.xlabel('x2')\n",
    "plt.title('Decision Boundary on Training Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1], hue=y_test, marker='s')\n",
    "sns.lineplot(x=x1_decision_line, y=x2_decision_line, color='lightseagreen')\n",
    "plt.xlabel('x1')\n",
    "plt.xlabel('x2')\n",
    "plt.title('Decision Boundary on Testing Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Evaluate different instances of our classifier with the **Breast Cancer dataset**, [avaible on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
    "\n",
    "Suggestion for the experiments:\n",
    "- Fix a given seed (random_state) for reproducibility\n",
    "- Use 80% of the data for training, and 20% for testing - stratified sample\n",
    "- Compared methods:\n",
    "    - Our implementation with default parameters\n",
    "    - Our implementation after fine-tuning the `learning_rate`, `n_epochs`, and `alpha`\n",
    "    - [LogisticRegression from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (which is not implemented with Grad. Descent)\n",
    "    - [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit) with default parameters\n",
    "- Compute (at least) the F1-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS:** Only optimize our method against the default (non-optimized) baselines is not a fair comparison. One should also optimize at least the main hyperparameters from the baselines for a more fair comparison. But, this is a simple exercise! Don't worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
